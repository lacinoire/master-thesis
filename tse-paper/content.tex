

\textbf{Motivation.}
Continuous Integration (CI) has become a common practice in software
engineering~\cite{hilton2016usage}.  Many software projects use
CI~\cite{hilton2016usage,staahl2014modeling,beller2017oops} to detect
bugs early~\cite{vasilescu2015quality,duvall2007continuous}, improve
developer productivity~\cite{miller2008hundred,hilton2016usage} and
communication~\cite{downs2012ambient}.  CI builds produce logs which
report results of various sub-steps within the build.  These build logs
contain a lot of valuable information for developers and researchers---for
example, descriptions of compile errors, linter warnings or failed
tests~\cite{beller2017oops,seo2014programmers,vassallo2017a-tale}.

CI builds produce logs which report the results of the steps within the build. The information stored in build logs can and has
already been used for a variety of applications. One prominent dataset
in the space, TravisTorrent, was featured as the MSR Data Challenge
2017~\cite{msr17challenge}. To date, scientist and practitioners alike
have used it and other, proprietary datasets to understand and
cateogrize Continuous Integration build
failures~\cite{islam2017insights}, to do research on testing
practices~\cite{orellana2017differences}, to train classifiers to
predict the build outcome and
duration~\cite{ni2017cost,bisong2017built,machalica2019predictive},
and to investigate static analysis tools in CI
builds~\cite{zampetti2017open}. Therefore, being able to efficiently
and correctly extract information from build logs is paramount to the
future of a variety of fields depending on it.

However, build logs can be verbose and large---sometimes in excess of
50 MB of ASCII text ~\cite{beller2017oops}---making them inadequate
for direct human consumption. Therefore, to support developers and
researchers in efficiently making use of the information within build
logs, we must at least semi-automatically retrieve the chunks of the
log that describe the targeted information.

There are different techniques to retrieve information chunks from CI
build logs. Beller et al. use a rule-based system of regular
expressions to analyze logs from Travis CI~\cite{beller2017oops}.
Such regular expressions are developed by looking at exemplary build
logs.  Vassallo et al.\ wrote a custom parser to gather information
for build repair hints~\cite{vassallo2018un-break}.  Recently, Amar et
al.\ reduced the number of lines for a developer to inspect by
creating a diff between logs from failed and successful
builds~\cite{amar2019mining}.

These approaches have various strengths and weaknesses: Regular
expressions are exact, but tedious and error-prone to
maintain~\cite{michael2019regexes}.  Custom parsers are powerful
though fragile in light of changes in the log structure. Diffing
between failed and successful logs can reduce the information to be
processed, but is at best semi-automatic~\cite{amar2019mining}.

At the moment there is only anecdotal evidence on the performance of these techniques and on when a technique should be preferred over other alternatives.
Developers and researches currently have little support when choosing which technique to use for a task.

The goal of this paper is to investigate different chunk retrieval techniques for build logs and describe under which circumstances certain techniques can be recommended over others.
We aim to characterize different chunk retrieval techniques.
For \textbf{Research Question 1} (\textbf{RQ1}), we analyze which criteria influence the suitability of a chunk retrieval technique for CI build logs.

We implement and evaluate three chunk retrieval techniques:
\begin{itemize}
  \item program synthesis by example using the Microsoft PROSE library (referred to as PBE),
  \item a common text similarity approach (referred to as CTS), and
  \item keyword search (referred to as KWS).
\end{itemize}
\textbf{RQ2} asks under which conditions PBE, CTS and KWS are suited to retrieve information from continuous integration build logs.
\textbf{RQ2} is refined into sub-questions along with the criteria resulting from \textbf{RQ1} and compares their instantiations for the three techniques:
how many training examples a technique needs to perform best (\textbf{RQ2.1}), how structurally diverse the examples can be (\textbf{RQ2.2}) and how accurate the retrieved output is (\textbf{RQ2.3}).
To evaluate PBE, CTS and KWS we use the \emph{LogChunks} data set, which encompasses about 800 log files from 80 repositories.
Each log is labeled with the log part describing the reason a build failed, keywords to search for this log part and a categorization of the labeled log part according to its structural representation within the log.

Our study of the three techniques on \emph{LogChunks} shows that
\begin{itemize}
  \item PBE yields very accurate results when trained with two examples from a single structural category.
  \item CTS shows the best average precision, though precision and recall of a retrieval is hard to determine from the given result.
  A small increase in the number of training examples has no noticeable influence.
  Fewer structural categories improve precision and recall of the retrieval.
  \item KWS has the highest recall of all techniques, however much lower precision.
  It is the technique with the best recall when multiple structural categories are present in the training examples.
\end{itemize}

\noindent
\textbf{Our work contributes:}
\begin{itemize}
  \item A tool unifying several chunk retrieval techniques namely:
        \begin{itemize}
          \item program synthesis from examples using the Microsoft PROSE library (PBE),
          \item a common information retrieval approach using text similarity (CTS), and
          \item a keyword search approach (KWS).
        \end{itemize}
  \item Recommendations for the configuration of each of the investigated chunk retrieval techniques.
  \item Guidelines on choosing a suitable chunk retrieval technique.
\end{itemize}

We recommend PBE for use cases where the desired information is always
represented in the same structural way and high confidence in precision and
recall of the chunk retrieval is required.
CTS is well suited when the representation of the desired information varies
slightly and the output of the chunk retrieval is further processed by a human.
In cases where the textual representation of the desired information in the log
is unpredictable or varies greatly, KWS is the best technique to choose.
However, its low precision requires a human to interpret the output of the chunk
retrieval.

\todo{structure overview of article}

\section{Related Work}
\label{sec:rw}
We explain how researchers gather information about CI usage in software projects
through build log analysis and why the chunk retrieval techniques we investigate
simplifies their data collection.
This section moves on to past works about augmenting build logs to make it easier
for developers to inspect them.
The presented approaches are related to and can benefit from the techniques we
analyze in this paper.
Further, we classify build logs as semi-structured data and differentiate our
work from system log analysis.

\subsection{Build Failures in CI}
Various researchers look into why CI builds fail and into the impact of build failures on the development workflow.

Seo et al.~\cite{seo2014programmers} find that a small group of error types such as dependency mismatches are the most prominent cause of build failures at Google.
In addition, they notice that most failures are resolved within two builds.
They based their analyses on sets of build logs collected from industry partners.
They develop a custom parser to classify error messages reported by Java and C++ builds.

Rausch et al.~\cite{rausch2017empirical} analyze CI builds of open source Java projects and find that most builds fail because of failing tests.
For most projects, over half of the failed builds follow a previous failed build.
Rausch et al.'s data shows that most failures occur in the second half of the build runtime, which can cause long delays in the feedback loop, especially when builds are automatically retried upon failure.

Vassallo et al.~\cite{vassallo2017a-tale} compare open source projects in Java to industrial ones.
They determine that testing failures are more common than compilation errors.
Open source builds fail most often because of unit tests, whereas release preparations are the primary cause in industrial projects.
Their data stems from analyzing build logs from \emph{TravisTorrent}~\cite{beller2017travistorrent} with regular expressions.

Ghaleb et al.~\cite{ghaleb2019studying} aim to identify noise in build breakage data.
They classify build failures from \emph{TravisTorrent} according to whether they were caused by an environmental failure or caused by a developer change.
Their analysis starts with manual categorization of build logs.
They select keywords and strings that identify their targeted categories and code a script to automatically classify logs based on these keywords.

\noindent
\textbf{Supporting Log Analysis with Chunk Retrieval}
To leverage the valuable information within build logs the researchers presented
in this section create parsers and regular expression-based programs.
This task of retrieving specific chunks of text from the build logs can be
solved by the chunk retrieval techniques we compare in this paper.
Our results can support researchers in choosing a suitable technique for their
data set of build logs and the chunks they want to retrieve.
By relieving them from building custom parsers we enable them to cover a much
wider range of languages and build tools in their studies.


\subsection{Augmentation of Build Logs}
\label{sec:rw-bl-analysis}
Build logs are a valuable data source for developers to find out why their build failed.
Several researchers are looking into supporting developers to process the verbose build logs.
Vassallo et al.~\cite{vassallo2018un-break} try to shorten the time it takes developers to understand build logs.
They parse Maven~\cite{maven2019website} build logs into a structured representation and create hint generators.
%The hint generators leverage this structured access to the information within the build log to propose fixes.
For example, one of the hint generators queries stack overflow for discussions related to why the build failed.
In a qualitative study they observed that highlighting the locality and context of an issue is helpful to programmers.
%Their tool BART is published as a Jenkins Plugin~\cite{bart2019plugin}.
The chunk retrieval techniques we compare in this paper can be used to fill similar structured representations with information from build logs.
As they simplify the construction of parsers they would enable developers and researchers to cover a wider array of build tools, which is the main influence factor on the structure of a build log.

Amar et al.~\cite{amar2019mining} compare different approaches to reduce the portions of a log that a developer has to inspect.
Their techniques remove lines that appear both in logs from passing and failing builds and use a modified \emph{Term Frequency Inverse Document Frequency} (TF-IDF) weighting to identify term vectors likely to occur with failures.
Their diff technique can be interpreted as a chunk retrieval technique, where the targeted information is defined by the past failures used as basis for the weighted term vectors.

Travis itself already provides build log augmentation in the form of a basic structuring of the build logs within their log viewer using \emph{log folds}~\cite{travis2019logfolds}.
They add fold identifiers around common commands and setup or teardown build phases and collapse the contained lines by default.

\subsection{System Log Analysis}
\label{sec:log-analysis}
A related field of log processing is the processing of system log files produced during runtime.
A main difference between build logs and system logs is that system logs are fundamentally structured through events.
Each line in a log file represents one event with a set of fields: timestamp, verbosity level and raw message content~\cite{he2017towards}.
Figure \ref{lst:system-log} shows example lines from a system log.

The first goal in parsing system log files is to separate constant and variable parts within a log message~\cite{nagappan2010abstracting,he2017towards}.
Next, the log messages are clustered into log events, unifying messages with identical constant parts and varying parameters.
The output of a log parser is a structured log, composed of a list of timed events and the corresponding parameter values~\cite{he2016evaluation}.
This structured log is then the input to various machine learning and data mining processes.
%Researchers mine patterns for operational profiling~\cite{nagappan2009efficiently}, debugging~\cite{oliner2012advances}, performance analytics or anomaly detection~\cite{nagappan2010abstracting}.
%Xu et al.~\cite{xu2009detecting} leverage the connection of log statements to the source code producing them to separate messages into constant and variable parts more accurately.

The techniques developed for system log analysis can also be applied to build logs.
One example is comparing execution traces to reference traces of intended behavior to detect anomalies.
Amar et al.~\cite{amar2019mining} employed a similar approach to detect relevant lines in build logs.
%Classic log parsers interpret the whole log file into a sequence of events.
%A similar approach could also be applied to build logs to determine the sequence of executed build steps or phases.
In this paper we focus on extracting a single specified information from the build log as a whole with chunk retrieval techniques.
Chunk retrieval techniques are used as a part of log parsing to retrieve the values of variable parts in a log message, e.g.\ by using regular expressions~\cite{nagappan2010abstracting,xu2009detecting}.

\lstset{
  morekeywords={INFO, WARN, 2008-11-09},
	basicstyle=\scriptsize,
  postbreak=\mbox{\textcolor{cyan}{$\hookrightarrow$}\space},
  showspaces=false,
  showstringspaces=false,
  keywordstyle=\color{blue},
	frame=single,
  extendedchars=false,
  texcl=false
}
\begin{figure}[!t]
  \centering
  \begin{lstlisting}[breaklines=true]
2008-11-09 20:46:55,556 INFO dfs.DataNode$Packet- Responder: Received block blk_3587508140051953248 of size 67108864 from /10.251.42.84
2008-11-09 20:49:46,764 WARN PacketResponder 0 for block blk\_3587508140051953248 terminating
  \end{lstlisting}  
  \caption{System Log Statements. Example adapted from~\cite{he2017towards}.}
  \label{lst:system-log}
\end{figure}

\subsection{Program Synthesis by Example}
\label{sec:rw-prose}
% \section{Information Extraction and Retrieval Techniques}
%The techniques we investigate are based on existing methods of Programming by Example.%, information extraction and information retrieval.
%This section presents different Programming by Example resources surrounding the PROSE library.
%We explain its generic program synthesis algorithm and how PROSE synthesizes text extraction programs, the foundation of PBE\@.

Programming by Example enables end users to automate repetitive tasks.
The user provides examples for the input and the corresponding output and a
synthesis algorithm tries to create the program intended by the user.
This section introduces the theoretical foundations of the program synthesis
algorithm of the \emph{PROgram Synthesis using Examples} (PROSE) framework~\cite{prose2019webpage}.
The PROSE framework is developed by Microsoft Research.
Next, this section presents the FlashExtract DSL, which defines text extraction
tasks within PROSE and is the basis for the implementation of our chunk retrieval technique PBE\@.

\subsubsection{FlashMeta: Inductive Program Synthesis}
The FlashMeta framework presented by Polozov and Gulwani~\cite{polozov2015flashmeta:} is the backbone of the program synthesis in the Microsoft PROSE framework.
FlashMeta separates the inductive synthesis algorithm from the domain specific capabilities of the desired program by encoding the possible program space in a domain specific language (DSL).
The user specifies the desired program behavior by providing in/output examples (I/O examples).
FlashMeta uses \emph{witness functions}, provided by the DSL, to divide the synthesis into smaller subtasks.
For each of these subtasks it enumerates all possible programs that solve the subtask consistent with the set of I/O examples.
A program is consistent with a set of I/O examples if, for each input example, it produces the corresponding output~\cite{mitchell1982generalization}.
The possible subprograms are joined and stored in a \emph{version space algebra} (VSA)~\cite{mitchell1982generalization}.
This a tree structure, which space-efficiently saves candidate programs for tasks by sharing common subexpressions.
Next, FlashMeta ranks the enumerated programs according to which ones the user most likely intended.
The DSL also provides the ranking characteristics.
From the ranked VSA, FlashMeta can then return a ranked list of complete programs consistent with the user's example.

% In addition to I/O examples of the intended program, the user can also provide examples with only input or negative input examples.
% Negative input examples should not be processed by the synthesized program.

% The different applications of PROSE presented in the following paragraphs were all eventually implemented as DSLs for the FlashMeta synthesis algorithm.

\subsubsection{FlashExtract: Data Extraction by Example}
Le et al.~\cite{le2014flashextract:} developed FlashExtract as a DSL for the Microsoft PROSE framework.
% It enables a user to define text extraction programs for text, websites and spreadsheets by giving I/O examples.
FlashExtract's instantiation for text synthesizes extraction programs from semi-structured text based on regular expressions.
Users can extract multiple fields and structure them with hierarchy and sequence.
% FlashExtract synthesizes programs to extract each of the fields leveraging the information about hierarchical containment and sequentiality.
% It eliminates the need for the user to understand the entire structure of the processed document and decreases the effort of developing a suitable extraction program.

FlashExtract models the extraction of a single substring as a pair of two cut positions.
A position is either specified by an absolute character index or by a pair of two regular expressions.
The first regular expression matches the substring directly before the characterized position, the second regular expression matches the substring directly after.
A regular expression in FlashExtract is a concatenation of tokens, e.g.\ standard character classes or string literals frequently occurring in the input examples.
Figure \ref{lst:prose-program} shows a text extraction program synthesized by FlashExtract.
This program defines the first position as after a colon followed by a newline character and before a piece of text with all capital letters.
It defines the second position as before two newline characters.

Apart from automatic completion in Excel spreadsheets~\cite{excel2019flashfill}, FlashExtract is the basis for two other Microsoft product features:
Microsoft's system log analysis tool Azure Monitor lets users define custom log fields~\cite{azure2019custom}.
The ConvertFrom-String function in PowerShell allows a user to specify an example template to extract hierarchical data from a text document~\cite{powershell2019convert}.

We apply the text instantiation of FlashExtract to the domain of build logs with our chunk retrieval technique PBE\@.

\lstset{
  language=caml,
	basicstyle=\scriptsize,
  morekeywords={PosToEndRegion, RegexPosition, RegexPair, StartToPosRegion},
  postbreak=\mbox{\textcolor{cyan}{$\hookrightarrow$}\space},
  showspaces=false,
  showstringspaces=false,
  stringstyle=\color{blue},
  keywordstyle=\bfseries\color{black},
	frame=single,
  extendedchars=false,
  escapeinside=//
}
\begin{figure}[!t]
  \centering
  \begin{lstlisting}[breaklines=true]
let s = v in let s = PosToEndRegion(s, RegexPosition(s, RegexPair("Colon/$\circ$/Line Separator", "ALL CAPS"), 1)) in StartToPosRegion(s, RegexPosition(s, RegexPair("/$\varepsilon$/", "Line Separator/{\color{blue}$\circ$}/Line Separator"), 1))
  \end{lstlisting}  
  \caption{Text extraction program synthesized by FlashExtract.}
  \label{lst:prose-program}
\end{figure}

\section{Chunk Retrieval Techniques}
\label{sec:techniques}
This section introduces the concept of chunk retrieval techniques.
We use these techniques to extract text chunks from build logs that represent a specific, targeted information.
We present the three chunk retrieval techniques we investigate in this paper:
program synthesis by example (PBE), common text similarity (CTS) and keyword search (KWS).

\begin{figure}[!t]
	\centering
	\includegraphics[width=\columnwidth, clip]{img/build-overview.pdf}
	\caption{The different entities related to a CI build. \todo{mention in text or remove}}
	\label{fig:build-overview}
\end{figure}

\subsection{Characteristics of Chunk Retrieval Techniques}
\label{sec:blirt}
For this paper, we want to evaluate different techniques to retrieve information chunks from build logs, which we call \emph{chunk retrieval techniques}.
The techniques we investigate do not require to parse the structure of a whole build log, but focus on extracting just one specific information.

The user provides a \textit{configuration} that specifies which information the chunk retrieval should target and supplies the necessary information for the technique to identify the targeted information chunk in a build log.
Each chunk retrieval has a specific \textit{granularity}, i.e.\ the smallest retrievable text piece and uses a specific \textit{identification technique} to select the log parts it retrieves.
Each configuration addresses a specific \textit{scope}.
The scope can be a specific project, a tool involved in the build, a programming language or global, when configuring a retrieval technique for all possible build logs.
A \emph{run} of a chunk retrieval technique consumes a build log as a plain text file and produces a string output, which consists of substrings of the build log text.

The following sections of this section introduce the three chunk retrieval techniques we investigate: program synthesis by example (PBE), common text similarity (CTS), and keyword search (KWS).
Lastly we describe other techniques which can also be treated as chunk retrieval techniques.
Table~\ref{tab:ctr} shows a comparison of the presented techniques.

\begin{table}[]
\centering
\caption{Overview of the described chunk retrieval techniques.}
\begin{tabularx}{\columnwidth}{@{}XlXlX@{}} 
\toprule
Name                         & Acronym & Identification Technique                                   & Granularity & Configuration             \\ 
\midrule
Program Synthesis by Example & PBE     & Regular expression program                                 & Character   & In/output examples      \\
Common Text Similarity       & CTS     & TF-IDF \& cosine similarity, expected number of lines & Line        & Output examples           \\
Keyword Search               & KWS     & Keywords, expected number of lines                    & Line        & Keywords, context length  \\
Random Line Retrieval        & RLR     & Random sample                                              & Line        & Retrieval length          \\
Diff Approach                & ---     & Line not present in successful log, information retrieval  & Line        & Logs from failing and successful builds      \\
\bottomrule
\end{tabularx}
\label{tab:ctr}
\end{table}

\subsection{Program Synthesis by Example (PBE)}
\label{sec:expl-pbe}
% \emph{Programming by Example} is a technique which synthesizes programs according
% to in- and output examples provided by the user.
% It enables users to create programs without a priori programming knowledge~\cite{mayer2015user}.
% In the context of text extraction through regular expressions, Programming by Example relieves the developer from having to understand the whole document structure to solve a single extraction task~\cite{le2014flashextract:}.
% In this work, we refer to our interpretation of Programming by Example as \emph{PBE}\@.
% We investigate the suitability of PBE to retrieve information chunks from build logs.
% In the following, we explain the configuration and application of PBE to chunk retrieval from CI build logs.

\subsubsection{Configuration}
In/output examples are the main driver of Programming by Example, we refer to them
as \emph{examples}.
The \emph{input} is a whole build log, i.e.\ the whole text of the build log file.
The \emph{output} is a substring of the log file text, representing the substring that should be retrieved by the synthesized program when given the corresponding input file.
One or multiple examples, the training set, configure a chunk retrieval with PBE:
they define the substring of a build log that should be extracted.
The PROSE program synthesis then tries to construct a regular expression program consistent with all training examples.
% A program is consistent with an example if it returns the defined output when executed on the defined input~\cite{mitchell1982generalization}.
PBE reports an error back to the user if it could not synthesize a consistent program.
% The program synthesis builds on the FlashExtract DSL, which in turn uses the FlashMeta algorithm.
% Both are described in Section~\ref{sec:rw-prose}.

\subsubsection{Application}
A run of PBE takes a build log file as input and applies the synthesized regular expression program.
It then returns the substring of the build log matched by the program or an empty string if the program found no match.

\subsection{Common Text Similarity (CTS)}
\label{sec:expl-ts}
% Text Similarity approaches are used to filter unstructured textual software artifacts~\cite{runeson2007detection,marcus2005recovery,antoniol2002recovering,mccarey2006recommending}.
% One common and simple technique is the Vector Space Model.
% We investigate when text similarity is a suitable technique to retrieve information chunks from build logs.
% In the following we will explain the concept of how we apply text similarity to information retrieval from CI build logs, which we refer to as \emph{CTS}\@.

\subsubsection{Configuration}
To configure chunk retrieval though text similarity we chose to use the same concept of examples as for PBE\@ and apply the Vector Space Model~\cite{schutze2008introduction}.
The lines of the output strings of the training examples define our search query.
The algorithm splits the search query into single lines and identifies tokens, in our case words.
Then we build a document-term-frequency matrix over the lines from the search query and prune very often or very rarely appearing words.
Next, the algorithm applies TF-IDF to the matrix, a best practice for natural language queries~\cite{lee1997document}.

\subsubsection{Application}
To retrieve the desired information from a build log, we parse the whole text and process it in the same way as the output of the training examples.
The algorithm calculates the cosine similarity~\cite{korenius2007principal} to compare each line of the build log with each line of the search query.
After summing up the similarities of each build log line to all search query lines, we sort the build log lines in decreasing similarity.
The average number of lines in the outputs of the training examples determines how many of the most similar lines are returned as the output of the retrieval run.

\subsection{Keyword Search (KWS)}
\label{sec:expl-skws}
When developers are looking for a specific piece of information within a large amount of unstructured information, a first ad-hoc approach they use is searching for related keywords.
Indeed, this was one of the most common approaches we took when searching for the reason the build failed within a log while creating the \emph{LogChunks} data set.
% As this is a technique readily available in many tools developers use to view build logs, we study when such a keyword search is suitable for retrieving information chunks from CI build logs.
% In the following we will explain how we use simple keyword search to retrieve information from CI build logs, which we refer to as \emph{KWS}\@.

\subsubsection{Configuration}
A set of keywords configures the chunk retrieval with KWS\@.
To better compare KWS with PBE and CTS, we also configure it through examples.
We link each example with keywords, which appear in the targeted chunk or close to it in the input build log.
The configuring keywords for KWS are the ones that appear most often in the keywords of all training examples.

\subsubsection{Application}
For a retrieval run, we take a whole build log file as input and search for all exact occurrences of the keywords.
As keywords are often not directly describing the desired information, but rather appear close to the desired information, KWS also retrieves the lines around the found keyword.
The number of surrounding lines retrieved is the average of lines in the output of the training examples.

\subsection{Other Techniques}
\label{sec:expl-rlr}
%Literature mentions further build log analysis techniques.
%This section describes them with our notion of chunk retrieval techniques.

\noindent
\textbf{Log Diff}
Amar et al.\ use a technique based on line diffs and information retrieval to identify relevant lines from a failed build log~\cite{amar2019mining}, as we describe in more detail in Section~\ref{sec:rw-bl-analysis}.
The configuration for the technique is the log from the last successful build and relevant past failures.
This technique retrieves the lines from a build log that are not present in the successful build log and contain terms related to the given past failures.

\noindent
\textbf{Random Line Retrieval (RLR)}
In our evaluation, we want to compare against a baseline of randomly extracted lines.
The average number of lines in the outputs of the training examples is the configuration for random line retrieval (RLR).
It retrieves this number of lines randomly sampled from the build log.

\subsection{Tool Implementation}
For our comparison study we implemented PBE, CTS, KWS and RLR and a unifying interface.
The unified interface is implemented in Ruby and calls the separate technique implementations over the command line.
The implementation of PBE in C\# is based on the Microsoft PROSE library~\cite{prose2019webpage}.
We implemented CTS, KWS and RLR using R and the text2vec library~\cite{text2vec2019webpage}.


\section{Study Design}
\label{sec:study}
To investigate when PBE, CTS and KWS are suited to retrieve chunks from CI build
logs we evaluate them on the \emph{LogChunks}~\cite{brandt2020logchunks} data set.
This section describes our study design and which metrics we measure to answer our research questions.
In the presentation of the results, we first focus on each of the three techniques and later compare them against each other.

\subsection{LogChunks}
For this study we created the previously published data set \emph{LogChunks}.
It encompasses 797 build logs from Travis CI, stemming from a broad range of 80
GitHub repositories and 29 programming languages. For each file, the authors
manually ``labeled the log part (chunk) describing why the build failed''~\cite{brandt2020logchunks}.
We include keywords, which we would use to search for the selected chunk
within the log. In addition, we categorized the log chunks according to their
format within the log. If the chunks are surrounded by the same markings within
the log we assign them the same structural category.

\begin{figure}[!t]
	\centering
	\includegraphics[page=2, width=\columnwidth, trim={0cm 1cm 1.5cm 0.5cm}, clip]{img/overview-graphics.pdf}
	\caption{Study design of our technique comparison study.}
	\label{fig:study}
\end{figure}


\begin{simplebox}{Research Questions}
\begin{itemize}
  \item[\textbf{RQ1:}] Which criteria influence the suitability of a chunk retrieval technique for CI build logs?
  \item[\textbf{RQ2:}] Under which conditions are PBE, CTS, and KWS suited to retrieve information from CI build logs?
  \item[\textbf{RQ2.1:}] How many examples do PBE, CTS, and KWS need to perform best?
  \item[\textbf{RQ2.2:}] How structurally similar do the examples for PBE, CTS and KWS need to be for the techniques to be applicable?
  \item[\textbf{RQ2.3:}] How accurate are the retrievals of PBE, CTS, and KWS?
\end{itemize}
\end{simplebox}

For the comparison, we evaluate the three chunk retrieval techniques PBE, CTS and KWS, described in Sections~\ref{sec:expl-pbe}, \ref{sec:expl-ts} and \ref{sec:expl-skws}.
Random Line Retrieval (RLR), explained in Section~\ref{sec:expl-rlr}, acts as a baseline for the comparison.
% We run four techniques on the examples from \emph{LogChunks}.

\noindent
\textbf{Training and Test Set}
We use \emph{LogChunks} as the data set for our study.
For each one of 80 repositories it contains about 10 build logs, manually labeled with the substring describing the reason the build failed.
In addition to that, it contains keywords to search for that substring and which structural category the substring belongs to.

For each repository in \emph{LogChunks}, we split the examples chronologically into training and test set.
Therefore, we train on examples from past build logs and test on more recent build logs.

\noindent
\textbf{RQ 2.1: Size of Training and Test Set}
To analyze how many examples the chunk retrieval techniques need to perform best, we evaluate the techniques with different training set sizes.
We train each technique with one to five examples from each of the repositories within \emph{LogChunks}.
The size of the test set is one.

\noindent
\textbf{RQ 2.2: Recording Structural Categories}
To determine how structurally similar the examples for the chunk retrieval techniques need to be, we record the structural categories of the examples in the training and test sets.

\noindent
\textbf{RQ2.3: Accuracy Metrics}
To measure the accuracy of the retrieved chunks we save the output lines of the chunk retrieval run on the input of the test example ($\mathit{RetrievedLines}$).
As oracle in our evaluation, we save the desired lines from the output of the test example ($\mathit{DesiredLines}$).

We calculate the following metrics:
\begin{itemize}
	 \itemsep0.75em
	\item True positives: $\mathit{DesiredLines} \cap \mathit{RetrievedLines}$
	\item Precision: $\dfrac{\# \mathit{TruePositives}}{\# \mathit{RetrievedLines}}$
	\item Recall: $\dfrac{\# \mathit{TruePositives}}{\# \mathit{DesiredLines}}$
	\item F$_{1}$-score: $2 \cdot \dfrac{\mathit{Precision} \cdot \mathit{Recall}}{\mathit{Precision} + \mathit{Recall}}$
	\item Successful retrieval: $\mathit{true}\ \mathit{if}\ \mathit{Recall} = 1$
\end{itemize}

Precision of a chunk retrieval describes which proportion of the retrieved lines were desired.
Recall of a chunk retrieval describes which proportion of the targeted lines were retrieved.
% Throughout the presentation and discussion of our results we show these two metrics separately, as they might have a different weight when choosing a suitable technique for a task.
In addition, we calculate the F$_{1}$-score, the harmonic mean of precision and recall.
We define a successful retrieval as one where all desired lines were extracted, therefore when recall is one.

% Recall and precision of CTS and KWS vary with the number of lines selected for retrieval.
% We evaluate the effect of varying the number of extracted lines by multiplying the average number of lines present in the training examples with a \emph{retrieval size factor} from 0.5 to 2.5 in steps of 0.5.


\section{Results}
This section presents the results for PBE, CTS and KWS separately.
Afterwards we compare the three techniques with each other and RLR as baseline.

\begin{figure}[!t]
		\centering
		\includegraphics[width=0.75\columnwidth, clip]{img/big-study/failure-reason-PBE.pdf}
		\caption{Results of chunk retrieval with PBE.}
		\label{fig:failure-reason-PBE}
\end{figure}

\lstset{
  morekeywords={Test Output, Desired Test Output},
	basicstyle=\scriptsize,
  postbreak=\mbox{\textcolor{cyan}{$\hookrightarrow$}\space},
  showspaces=false,
  showstringspaces=false,
  keywordstyle=\color{blue},
	frame=single,
  extendedchars=false,
  texcl=false
}
\begin{figure}[!t]
  \centering
  \begin{lstlisting}[breaklines=true]
Test Output:
Error: Invalid CSS after "2.3em": expected expression (e.g. 1px, bold), was ";"
        on line 86 of sass/components/dropdown.sass   
Desired Test Output:
Error: Invalid CSS after "2.3em": expected expression (e.g. 1px, bold), was ";"
        on line 86 of sass/components/dropdown.sass
        from line 5 of sass/components/_all.sass
        from line 6 of bulma.sass
  \end{lstlisting}  
  \caption{Example for an unsuccessful retrieval (PBE retrieved only two of the four targeted lines).}
  \label{lst:pbe-unsuccessful}
\end{figure}


\subsection{Program Synthesis by Example (PBE)}
Figure~\ref{fig:failure-reason-PBE} shows the results of the PBE runs in our evaluation.
Out of the 400 runs, 5 per each one of the 80 example sets, PBE extracted all the desired lines in 138 cases.
In 89 further cases a program was also successfully synthesized, though in 59 of the 89 cases the synthesized program yielded no output at all.
In 30 of the 89 cases the synthesized program did not extract all of the desired lines.
For these 30 cases, the average recall was 28\%.
We present an example of such an \emph{unsuccessful retrieval} in Listing \ref{lst:pbe-unsuccessful}, where the synthesized regular expression program only retrieved two of the four targeted lines.
In 173 of the 400 cases the PROSE program synthesis could not synthesize a regular expression program that is consistent with all of the training examples.

Figure~\ref{fig:failure-reason-categorycount-PBE} shows the results of PBE runs compared to the number of structural categories present in the training and test examples.
It shows that the program synthesis is more likely to succeed when there are only a few categories present in the training examples.
When two structural categories are present, PROSE could in most cases not synthesize a program consistent with all training examples.
For three or more present categories PROSE could never synthesize a consistent program.

Figure~\ref{fig:recall-precision-examplecount-sythesisworked-PBE} shows precision and recall of the 227 runs where PBE could synthesize a program consistent with all training examples.
When the training set size increases from one to two, recall and F$_{1}$-score increase by about 25\%, precision increases by about 10\%.
For two or more training examples, recall and F$_{1}$-score stay around 75\% and precision around 96\%.


\subsection{Common Text Similarity (CTS)}
Figure~\ref{fig:recall-precision-examplecount-CTS} presents precision, recall and F$_{1}$-score of chunk retrieval using CTS for an increasing number of training examples.
When using one to five training examples, the size of the training set has no noticeable influence on precision, recall or F$_{1}$-score of the chunk retrieval with CTS.

Figure~\ref{fig:recall-precision-categorycount-CTS} shows the same measurements for an increasing number of structural categories in the training and test examples.
With increasing category count, precision, recall and F$_{1}$-score decrease.
Especially for more than three categories present we have no chunk retrieval runs where all desired lines were extracted.

% Figure~\ref{fig:contextsizefactor-precision-recall-CTS} shows the effect of the retrieval size factor on precision, recall and F$_{1}$-score of chunk retrieval runs with CTS\@.
% The precision ranges from 52\% when retrieving half expected number of lines to 25\% when 2.5 times the expected number of lines.
% The recall ranges from 30\% to 55\%.
% A retrieval size factor of 1 gives the best average F$_{1}$-score with 51\%.

\begin{figure}[!t]
		\centering
		\includegraphics[width=\columnwidth, clip]{img/big-study/failure-reason-categorycount-PBE.pdf}
		\caption{Results of chunk retrieval with PBE for an increasing number of structural categories in the training and test sets.}
		\label{fig:failure-reason-categorycount-PBE}
\end{figure}

\begin{figure}[!t]
		\centering
		\includegraphics[width=\columnwidth, clip]{img/big-study/recall-precision-examplecount-sythesisworked-PBE.pdf}
		\caption{Precision, recall and F$_{1}$-score of chunk retrieval when PBE could synthesize a consistent program compared with the size of the training set.}
		\label{fig:recall-precision-examplecount-sythesisworked-PBE}
\end{figure}

\begin{figure}[!t]
		\centering
		\includegraphics[width=\columnwidth, clip]{img/big-study/recall-precision-examplecount-CTS.pdf}
		\caption{Precision, recall and F$_{1}$-score of chunk retrieval with CTS for an increasing training set size.}
		\label{fig:recall-precision-examplecount-CTS}
\end{figure}

\begin{figure}[!t]
		\centering
		\includegraphics[width=\columnwidth, clip]{img/big-study/recall-precision-categorycount-CTS.pdf}
		\caption{Precision, recall and F$_{1}$-score of chunk retrieval with CTS for an increasing number of structural categories in the training and test sets.}
		\label{fig:recall-precision-categorycount-CTS}
\end{figure}

% \begin{figure}[!t]
% 		\centering
% 		\includegraphics[width=\columnwidth, clip]{img/big-study/contextsizefactor-precision-recall-CTS.pdf}
% 		\caption{Precision, recall and F$_{1}$-score of chunk retrieval with CTS compared to retrieval size factors.}
% 		\label{fig:contextsizefactor-precision-recall-CTS}
% \end{figure}


\subsection{Keyword Search (KWS)}
Figure~\ref{fig:recall-precision-examplecount-KWS} presents precision, recall and F$_{1}$-score of chunk retrieval using KWS for different numbers of training examples.
The recall increases by about 12\% when increasing the size of the training set to more than one example, while the precision stays constant at around 16\%.
The F$_{1}$-score stays around 26\%.

Figure~\ref{fig:recall-precision-categorycount-KWS} shows the same measurements for an increasing number of structural categories in the training and test examples.
For more than one structural category in the training and test examples the recall decreases by about 20\% and the precision decreases about 6\%.
For more than two structural categories no clear trend is visible in precision, recall or F$_{1}$-score for an increasing amount of categories in the training and test examples.

% Figure~\ref{fig:contextsizefactor-precision-recall-KWS} shows the effect of the retrieval size factor on precision, recall and F$_{1}$-score of chunk retrieval runs with KWS\@.
% The precision is 19\% when retrieving half of the expected number of lines. On average 9\% of the lines in the build log are retrieved then.
% When 2.5 times the expected number of lines are retrieved, the precision decreases to 10\% and a quarter of the lines in the build log are retrieved on average.
% The recall ranges from 58\% to 75\% and the F$_{1}$-score shows a constant decrease from 29\% to 17\%.

\begin{figure}[!t]
		\centering
		\includegraphics[width=\columnwidth, clip]{img/big-study/recall-precision-examplecount-KWS.pdf}
		\caption{Precision, recall and F$_{1}$-score of chunk retrieval with KWS for an increasing training set size.}
		\label{fig:recall-precision-examplecount-KWS}
\end{figure}

\begin{figure}[!t]
		\centering
		\includegraphics[width=\columnwidth, clip]{img/big-study/recall-precision-categorycount-KWS.pdf}
		\caption{Precision, recall and F$_{1}$-score of chunk retrieval with KWS for an increasing number of structural categories in the training and test sets.}
		\label{fig:recall-precision-categorycount-KWS}
\end{figure}

% \begin{figure}[!t]
% 		\centering
% 		\includegraphics[width=\columnwidth, clip]{img/big-study/contextsizefactor-precision-recall-KWS.pdf}
% 		\caption{Precision, recall and F$_{1}$-score of chunk retrieval with KWS compared to retrieval size factor.}
% 		\label{fig:contextsizefactor-precision-recall-KWS}
% \end{figure}

\subsection{Comparison of All Techniques}
Figure~\ref{fig:success-partial-all} compares the success of all chunk retrieval by the different techniques in our study.
CTS and KWS extract some of the desired lines in 79\% and 88.5\% of the chunk retrieval runs.
With 38.25\%, KWS also has the highest number of fully successful extractions, followed by PBE with 34.5\%.
PBE has the lowest number of partial retrievals with only 18 out of 400 chunk retrieval runs.

% The averaged precision, recall and F$_{1}$-score f all techniques is compared in Figure~\ref{fig:recall-precision-all}.
% The recall of PBE has a high skew towards one and zero, meaning in most cases either the retrieval is successful or no relevant lines are extracted at all.
% PBE has the highest average precision with 95\%.
% Chunk retrieval with CTS has the highest average F$_{1}$-score with 51\% and the second highest recall with 46\%.
% KWS has the smallest precision of the three chunk retrieval techniques.
% With 16\% it is still higher than the precision of the RLR baseline with 7\%.
% KWS has the highest recall of all techniques with 70\%.

Figure~\ref{fig:recall-precision-singlecategory-all} and Figure~\ref{fig:recall-precision-multicategory-all} show the influence of a single structural category present in the training examples compared to multiple categories present.
For more than one present category, the recall of PBE decreases greatly.
For CTS and KWS the values also decrease, while RLR is not affected by the number of structural categories present.

\begin{figure}[!t]
		\centering
		\includegraphics[width=\columnwidth, clip]{img/big-study/success-partial-all.pdf}
		\caption{Success of chunk retrievals for all techniques.}
		\label{fig:success-partial-all}
\end{figure}

% \begin{figure}[!t]
% 		\centering
% 		\includegraphics[width=\columnwidth, clip]{img/big-study/recall-precision-all.pdf}
% 		\caption{Precision, recall and F$_{1}$-score of all techniques compared.}
% 		\label{fig:recall-precision-all}
% \end{figure}

\begin{figure}[!t]
	\centering
		\includegraphics[width=\columnwidth, clip]{img/big-study/recall-precision-singlecategory-all.pdf}
		\caption{Precision, recall and F$_{1}$-score of all techniques compared when training examples are in \emph{one} structural category.}
		\label{fig:recall-precision-singlecategory-all}
\end{figure}

\begin{figure}[!t]
		\centering
		\includegraphics[width=\columnwidth, clip]{img/big-study/recall-precision-multicategory-all.pdf}
		\caption{Precision, recall and F$_{1}$-score of all techniques compared when training examples are in \emph{more than one} structural categories.}
		\label{fig:recall-precision-multicategory-all}
\end{figure}


\section{Discussion}
\label{sec:discussion}

This section presents the answers to our research questions:
\begin{simplebox}{Research Questions}
\begin{itemize}
 \item[\textbf{RQ1:}] Which criteria influence the suitability of a chunk retrieval technique for CI build logs?
  \item[\textbf{RQ2:}] Under which conditions are PBE, CTS, and KWS suited to retrieve information from CI build logs?
  \item[\textbf{RQ2.1:}] How many examples do PBE, CTS, and KWS need to perform best?
  \item[\textbf{RQ2.2:}] How structurally similar do the examples for PBE, CTS and KWS need to be for the techniques to be applicable?
  \item[\textbf{RQ2.3:}] How accurate are the retrievals of PBE, CTS, and KWS?
\end{itemize}
\end{simplebox}

The first section discusses for PBE, CTS and KWS separately in which cases they perform best.
It details for which types of input build logs, available training examples and consumption of the retrieved output each technique is suited.
In the following section we discuss which of these criteria should influence the decision to use a certain technique most.
Based on our empirical comparison, we present a decision tree between the three techniques we investigated.

\subsection{Interpretation of Study Results}
This section discusses the study results for each of the analyzed chunk retrieval techniques separately.
It gives recommendations which kind of information chunk targets are best for each technique and for what kind of usage the respective output is suitable.

\begin{table}[tbp]
\centering
\begin{tabularx}{\columnwidth}{@{}llll@{}}
  \toprule
  & PBE & CTS & KWS \\
  \midrule
  Structural Categories & 1 & less is better & \makecell[l]{best 1 \\ multiple okay} \\
  Training Set Size & 2 & no influence & 2 \\ 
  Precision & \makecell[l]{high \\ (if synthesis succeeds)} & medium & low \\ 
  Recall & \makecell[l]{high \\ (if synthesis succeeds)} & medium & high \\ 
  \makecell[l]{Confidence in \\ Output Correctness} & high & low & \makecell[l]{low (precision) \\ high (recall)} \\ 
  Output Consumption by & program & human & human \\ 
  \bottomrule
\end{tabularx}
\caption{Recommendations for each of the investigated chunk retrieval techniques.}
\label{tab:single-technique-recommendations}
\end{table}

\subsubsection{Program Synthesis by Example (PBE)}

\noindent
\textbf{Configuration and Input}
Our study results show that chunk retrieval with PBE gives best results when the training examples are from one structural category.
This means it is suited to retrieve information chunks that always have the same surrounding structure.
To extract for example the reason a build failed, the log passage describing the failure would always have to be started and ended the same way.

When the training examples are of the same structure, two examples are enough input for PROSE to synthesize a regular expression program with good recall.
In our study, additional training examples did not improve the chunk retrieval.

\noindent
\textbf{Retrieval Output Usage}
If the program synthesis succeeds and applying the regular expression program yields an output, PBE shows a high precision and a high recall.
The tool clearly identifies a failing program synthesis or when no output from the program applied to a build log is obtained.
Therefore, if there is an output, the user can have high confidence that it is the correct output.
This makes output from PBE chunk retrieval well suited for consumption by other software components.

\subsubsection{Common Text Similarity (CTS)}
\noindent
\textbf{Configuration and Input}
Chunk retrieval using CTS also yields better results the fewer structural categories are present in the training and test examples.

The number of training examples had no noticeable influence on precision or recall in our study.
Information retrieval techniques like text similarity commonly learn on a higher number of examples than used for our study.
Future work is needed to investigate how many examples yield improvements in the chunk retrieval over a single training example.

Extracting the average number of lines present in the training examples gives the best retrieval results for CTS, according to the F$_{1}$-score.

\noindent
\textbf{Retrieval Output Usage}
CTS has good precision and recall on average, though the precision or recall of a chunk retrieval run is very hard to predict from the given result.
Therefore, retrieval output from CTS is suited to be read by a human.

\subsubsection{Keyword Search (KWS)}
\noindent
\textbf{Configuration and Input}
KWS has a higher recall than the two other techniques for multiple structural categories present in the training and test examples.
This makes KWS a good technique if there is little prior knowledge on how the targeted log chunk is represented in the build log to be analyzed.
For the example of extracting the reason the build failed, KWS is best suited if a build can fail in various steps logged by different tools and no pre-categorization of where the build failed is available.

With two training examples KWS achieves good recall.

Retrieving the average number of lines present in the outputs of the training examples around every found keyword yields reasonable recall.
Selecting 1.5 times as many lines around every found keyword does improve the recall within our study but also increases the proportion of lines retrieved overall and therefore decreases precision.

\noindent
\textbf{Retrieval Output Usage}
Even though KWS has the highest recall of all three techniques, its precision is also the lowest.
The output of a chunk retrieval with KWS is well suited to be read by humans.

\begin{figure}[!t]
		\centering
		\includegraphics[width=\columnwidth, clip]{img/crt-recommendation.pdf}
		\caption{Our preliminary recommendation scheme for chunk retrieval techniques.}
		\label{fig:crt-recommendation}
\end{figure}

\subsection{Recommendations of Suitable Techniques}
After discussing the three chunk retrieval techniques separately we now want to unify our results into one recommendation scheme.
In Figure~\ref{fig:crt-recommendation}, we present a decision tree, which developers and researchers who want to retrieve information chunks from build logs can follow.
The decision tree is built up of questions which either lead to more questions or to a leaf node containing a recommended technique. 

\noindent
\textbf{Caveat!} This is a preliminary theory based on the results from our comparison study.
The recommendations are therefore based on our implementation of the chunk retrieval techniques as well as the logs in the \emph{LogChunks} data set.

This decision tree is the answer to our first research question about which criteria influence the suitability of a chunk retrieval technique.
The earlier in the decision tree a criterion is noted, the more important it is when distinguishing the techniques.

The first and most important aspect are the structural categories.
Are the information chunks you would like to retrieve always presented in the same structural way within the build logs?
Then the information chunks in all training examples and the analyzed build log are in the same structural category.

If the information chunks are from multiple structural categories, i.e. they are not represented in the same structural way within the build logs, and recall is more important than precision we recommend to use KWS\@.
If the representations are from multiple structural categories and precision is more important than recall to the user we recommend CTS\@.
We also recommend CTS when the representations are from one structural category, when the user does not require a high confidence in the precision or recall of the outcome and when the user would rather have output with low recall instead of no output at all.
When the representations are from one structural category and the user wishes a high confidence in the correctness of the output or prefers no output over output with low recall, we recommend PBE\@.

\noindent
\textbf{Example of Using the Recommendation Scheme}
To illustrate how one would use our decision tree to find a suitable chunk retrieval technique we describe two concrete examples: a researcher investigating why CI builds fail and a software team wanting to monitor their build performance.

In our first example, a researcher studies whether test failures in CI are caused by a small or by a large group of test cases.
They gather CI build logs from various projects, which are their only available data source.
The task of the researcher is to extract the names of the failing test cases from each build log.
When they use our recommendation scheme to select a chunk retrieval technique, they first have to estimate how uniform the representation of the failing test cases is in the investigated build logs.
As the researcher is covering a wide range of build tools and development languages, the log chunks they target are in various, non-predictable structural representations.
The next question is whether they value precision over recall.
As they have to manually inspect the results of both CTS and KWS, they choose recall over precision to avoid having to inspect the whole log in case the relevant information chunk was not retrieved.
Therefore, our decision tree recommends them to use KWS\@.

In case the researcher wants to avoid manually inspecting the retrieval results, they have to first separate the CI build logs according to the test tool responsible for logging the test results.
Then the targeted log chunks are from one structural category and they can use PBE, trained with examples from each test tool separately.

In our second example, a software development team wants to monitor the performance of the phases within their CI build.
They are using Travis CI, which measures the duration of build phases and documents them within the build log.
As all log statements that report timing measurements are formatted the same way, the targeted log chunks are from one structural category.
Therefore the development team can use PBE to retrieve the duration of a build phase as well as its name.

\section{Threats to Validity}
There are several threats to the validity of the conclusions of our work.

\noindent
\textbf{Implementation}
Our results are highly dependent on our implementation of the investigated chunk retrieval techniques and the libraries we used.
Our implementation of PBE is severely based on the program synthesis provided by PROSE\@.
Its limitations are therefore also mainly influenced by this library.
For example, the need for examples from a single structural category stems from the fact that FlashExtract cannot learn regular expression programs with arbitrary boolean conjunctions and disjunctions~\cite{mayer2015user}.
This limitation was necessary to keep the synthesis performance reasonable.

Our implementation of CTS is dependent on the library tex2vec and the way they split strings into word tokens.
We intentionally chose a simple, minimally configured and tuned approach to compare against.
Tuning the text similarity meta-parameters more to the specific use case of chunk retrieval from build logs would yield better chunk retrieval results.

\noindent
\textbf{Data Set}
The outcomes of our comparison study are highly dependent on the build logs from the \emph{LogChunks} data set.
It only consists of build logs from open source projects and therefore it is not clear whether our results are generalizable to industry projects.
We only collected build logs from Travis CI, however we chose to evaluate on an information chunk whose format is not dependent on Travis CI\@.
This is because the reason the build failed is described within the build logs by the tools themselves and not the Travis CI environment.

\noindent
\textbf{Training Set Size}
Especially the results for CTS might be influenced by the fact that we only trained on one to five examples.
We chose this small training set size as the training examples have to be provided per repository and we expect a developer to not want to provide more examples than the small numbers we evaluated on.

\noindent
\textbf{Few Samples with Many Structural Categories}
Our comparison study shows fewer measurements with many structural categories than with one category.
This stems from the fact that we use a realistic data set which in many cases has only one or few structural categories.

\section{Conclusion and Future Work}
\label{sec:conclusion-fw}
The goal of this paper was to support researchers and developers in their decision on how to analyze build logs.
We implemented and compared three different chunk retrieval techniques on our data set \emph{LogChunks}, composed of 797 manually labeled build logs from a broad range of 80 repositories.
Our results show that the structural representation of the targeted information in the build logs is the main factor to consider when choosing a suitable technique.
Secondary factors are the desired confidence into recall and precision of the produced output and whether precision or recall is more important for the task at hand.

There are various future research opportunities based on our work:
\begin{itemize}
  \item \textbf{Further Analysis of \emph{LogChunks}} We created the \emph{LogChunks} data set specifically for our comparative study, though it can be the basis for various further analyses of build log data.
  The keywords, for example, can be investigated to answer which keywords are used to search for the reason the build failed within build logs.
  \item \textbf{Cross-Repository Build Log Analysis} We trained and tested each chunk retrieval technique on examples from the same repository.
  We propose to analyze how techniques could be trained across repositories, building the cornerstones for build environment-agnostic analysis tools.
  \item \textbf{Comparison with more Chunk Retrieval Techniques} This paper investigates the three chunk retrieval techniques PBE, CTS and KWS\@.
  Our study design can be reused to evaluate other build log analysis techniques, such as the diff and information retrieval approach by Amar et al.~\cite{amar2019mining}.
  \item \textbf{Refinement of Retrieval Quality for each Technique} We investigated basic configurations of existing techniques applied to chunk retrieval from build logs.
  In a next step, each of these techniques could be refined to better approach the domain of build logs.
  The \emph{LogChunks} data set and our study results act as a baseline to benchmark such technique improvements.
  We propose the following improvements:
    \begin{itemize}
      \item \textbf{Custom Ranking and Tokens for PBE} The program synthesis through PROSE ranks possible programs according to what the user most likely intended.
      One could adapt the ranking rules provided by the FlashExtract DSL to fit common build log chunk retrieval tasks.
      FlashExtract includes special tokens when enumerating possible regular expressions.
      One could extend these with tokens found in build logs, such as ``-'',``='',``ERROR'' or ``[OK''.
      \item \textbf{Meta-Parameter Optimization for CTS} Information retrieval techniques have various meta-parameters which can be optimized for the specific use case~\cite{panichella2016parameterizing}.
      We propose to further investigate improvements in preprocessing of the log text, in tokenization of the log lines into terms and in stop words lists.
    \end{itemize}
  \item \textbf{Usability Analysis of Chunk Retrieval Output} Our analysis of the output produced by the chunk retrieval focuses on precision and recall.
  We propose to investigate how useful these outputs are to developers through controlled experiments.
\end{itemize}

